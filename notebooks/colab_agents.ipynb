{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ1Y8bB5g8fC"
      },
      "source": [
        "Download the precompiled `llama-server` binary from the following github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z2l13h4fFei",
        "outputId": "77f5fc1c-8add-4ef5-d6fa-f6b06b190caa"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/jean-rl/llama-cpp-colab.git\n",
        "%cd llama-cpp-colab\n",
        "!chmod +x llama-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiLIA9w1C1gj"
      },
      "source": [
        "Then, download model weights in GGUF format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW-RADbMCykp",
        "outputId": "e1d04d46-3e40-4631-e82f-24acd2cba2e1"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWMCEVeuhvzO"
      },
      "source": [
        "Then open a terminal and run the following command.\n",
        "\n",
        "```\n",
        "./llama-server -m Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf --port 9090 -v --verbose-prompt --jinja -ngl 9999\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUq4jqLe6tDO"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_DINSXo6p2a"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI(\n",
        "    base_url=\"http://127.0.0.1:9090/v1\",\n",
        "    api_key = \"sk-no-key-required\"\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"Hi, how are you?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I am doing well, thank you! How can I assist you today?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you tell me a joke?\"},\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRpswiKx68Gh",
        "outputId": "30d5f1ab-9bb7-4d19-f3cb-386f85842317"
      },
      "outputs": [],
      "source": [
        "completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM9G5XzPDYfj",
        "outputId": "68a6f827-8626-4762-9793-bc4c0753f7ad"
      },
      "outputs": [],
      "source": [
        "print(completion.__verbose['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGbdm_UGDczX",
        "outputId": "6ef1b1d8-3873-409d-ea9e-d3261c3852d9"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Completions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = openai.OpenAI(\n",
        "    base_url=\"http://127.0.0.1:9090/v1\",\n",
        "    api_key = \"sk-no-key-required\"\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather today in Guanajuato?\"},\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-3vaysjro2tmqduo5xvr28d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I\\'m not able to access real time information, but I can suggest a few options to help you find out the current weather in Guanajuato:\\n\\n1. Check online weather websites: You can check websites such as accuweather.com, wunderground.com, or weather.com for the current weather conditions in Guanajuato.\\n2. Use a search engine: Simply type \"current weather in Guanajuato\" in your preferred search engine and you should get the latest information on temperature, humidity, wind speed, etc.\\n3. Check social media or local news sources: Many cities have local news sources or social media accounts that provide up-to-date information on the current weather.\\n\\nKeep in mind that the weather can change quickly, so it\\'s always a good idea to check multiple sources for the most accurate and up-to-date information.\\n\\nAs of my knowledge cutoff in 2023, Guanajuato has a temperate climate with mild winters and warm summers. The average temperature ranges from 10°C (50°F) in winter to 25°C (77°F) in summer.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1762239211, model='meta-llama-3.1-8b-instruct', object='chat.completion', service_tier=None, system_fingerprint='meta-llama-3.1-8b-instruct', usage=CompletionUsage(completion_tokens=221, prompt_tokens=46, total_tokens=267, completion_tokens_details=None, prompt_tokens_details=None), stats={})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"Hi, how are you?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I am doing well, thank you! How can I assist you today?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you tell me a joke?\"},\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_weather_schema =  {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Get current temperature for a given location.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\n",
        "                \"location\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The actual function will be used for calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_weather(location):\n",
        "    print(f\"Getting weather for {location}...\")\n",
        "    # Simulate a weather API call\n",
        "    return f\"The current temperature in {location} is 25°C.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_history = [\n",
        "        #{\"role\": \"system\", \"content\": \"You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal user question.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hi, what is the weather today in Bogota?\"}\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=chat_history,\n",
        "    tools=[get_weather_schema]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.tool_calls[0].function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "if completion.choices[0].message.tool_calls:\n",
        "    if completion.choices[0].message.tool_calls[0].function.name == 'get_weather':\n",
        "        response = get_weather(ast.literal_eval(completion.choices[0].message.tool_calls[0].function.arguments)['location'])\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = str({\"output\": response}) # If I don't return it in form of this dict. The model doesn't responds correctly. It's in the Llama documentation also https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_history.append({\"role\": \"assistant\", \"tool_calls\": completion.choices[0].message.tool_calls})\n",
        "chat_history.append({\"role\": \"tool\", \"content\": response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal user question.\"}] + chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=chat_history,\n",
        "    #tools=[get_weather_schema] # If I include it. The model also responds but this contaminates the prompt and could harm performance\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.tool_calls[0].function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Agent Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self):\n",
        "        self.chat_history = []\n",
        "\n",
        "    def listen(self, message):\n",
        "        self.chat_history.append({'role': 'user', 'content': message})\n",
        "        self._think()\n",
        "\n",
        "    def _think(self):\n",
        "        completion = client.chat.completions.create(\n",
        "            model='gpt-3.5-turbo',\n",
        "            messages=self.chat_history,\n",
        "        )\n",
        "        self._say(completion.choices[0].message.content)\n",
        "        self.chat_history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
        "\n",
        "    def _say(self, message):\n",
        "        print(f\"[INFO] The agent answered: {message}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "marvin = Agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "marvin.listen(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "marvin.chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "marvin.listen(\"Can you tell me a joke?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "marvin.chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, persona=None):\n",
        "        self.chat_history = []\n",
        "        if persona:\n",
        "            self.chat_history.append({\"role\": \"system\", \"content\": persona})\n",
        "\n",
        "    def listen(self, message):\n",
        "        self.chat_history.append({'role': 'user', 'content': message})\n",
        "        self._think()\n",
        "\n",
        "    def _think(self):\n",
        "        completion = client.chat.completions.create(\n",
        "            model='gpt-3.5-turbo',\n",
        "            messages=self.chat_history,\n",
        "        )\n",
        "        self._say(completion.choices[0].message.content)\n",
        "        self.chat_history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
        "\n",
        "    def _say(self, message):\n",
        "        print(f\"[INFO] The agent answered: {message}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren = Agent(persona=\"You are Warren Buffet, the famous investor. You are wise and give financial advice.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren.listen(\"Hello, what advice in a short sentence can you give me about investing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren.chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren.listen(\"What is the current stock price of Apple?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "def get_stock_price(ticker: str):\n",
        "    t = yf.Ticker(ticker)\n",
        "    info = t.info\n",
        "    return {\n",
        "        \"ticker\": ticker.upper(),\n",
        "        \"price\": info.get(\"currentPrice\"),\n",
        "        \"currency\": info.get(\"currency\"),\n",
        "        \"marketCap\": info.get(\"marketCap\"),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_stock_price(\"AAPL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_stock_price_schema = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_stock_price\",\n",
        "        \"description\": \"Get the current stock price and related financial information for a given ticker symbol.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"ticker\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Stock ticker symbol (e.g., AAPL, TSLA, MSFT).\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\n",
        "                \"ticker\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let it call tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, persona=None, tools=None):\n",
        "        self.chat_history = []\n",
        "        if persona:\n",
        "            self.chat_history.append({\"role\": \"system\", \"content\": persona})\n",
        "        if tools:\n",
        "            self.tools = tools\n",
        "\n",
        "    def listen(self, message):\n",
        "        self.chat_history.append({'role': 'user', 'content': message})\n",
        "        self._think()\n",
        "\n",
        "    def _think(self):\n",
        "        completion = client.chat.completions.create(\n",
        "            model='gpt-3.5-turbo',\n",
        "            messages=self.chat_history,\n",
        "            tools=self.tools \n",
        "        )\n",
        "        self._decide(completion)\n",
        "\n",
        "    def _decide(self, completion):\n",
        "        response = completion.choices[0].message.content if completion.choices[0].message.content else None\n",
        "        tool_call = completion.choices[0].message.tool_calls if completion.choices[0].message.tool_calls else None\n",
        "        if response is None and tool_call is None:\n",
        "            pass\n",
        "        if response is not None and tool_call is not None:\n",
        "            pass\n",
        "        if response is not None and tool_call is None:\n",
        "            pass\n",
        "        if response is None and tool_call is not None:\n",
        "            pass\n",
        "            \n",
        "    def _say(self, message):\n",
        "        print(f\"[INFO] The agent answered: {message}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, persona=None, tools=None):\n",
        "        self.chat_history = []\n",
        "        if persona:\n",
        "            self.chat_history.append({\"role\": \"system\", \"content\": persona})\n",
        "        if tools:\n",
        "            self.tools = tools\n",
        "\n",
        "    def listen(self, message):\n",
        "        self.chat_history.append({'role': 'user', 'content': message})\n",
        "        self._think()\n",
        "\n",
        "    def _think(self):\n",
        "        completion = client.chat.completions.create(\n",
        "            model='gpt-3.5-turbo',\n",
        "            messages=self.chat_history,\n",
        "            tools=self.tools \n",
        "        )\n",
        "        self._decide(completion)\n",
        "\n",
        "    def _decide(self, completion):\n",
        "        response = completion.choices[0].message.content if completion.choices[0].message.content else None\n",
        "        tool_call = completion.choices[0].message.tool_calls if completion.choices[0].message.tool_calls else None\n",
        "        if response is not None and tool_call is None:\n",
        "            self._say(completion.choices[0].message.content)\n",
        "            self.chat_history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
        "        if response is None and tool_call is not None:\n",
        "            print(\"[DEBUG] Tool call detected:\")\n",
        "            print(tool_call)\n",
        "        if response is not None and tool_call is not None:\n",
        "            print(\"[DEBUG] Both response and tool call detected, which is unexpected.\")\n",
        "            print(\"[DEBUG] Response:\", response)\n",
        "            print(\"[DEBUG] Tool call:\", tool_call)\n",
        "        if response is None and tool_call is None:\n",
        "            print(\"[DEBUG] No response or tool call detected, which is unexpected.\")\n",
        "            \n",
        "    def _say(self, message):\n",
        "        print(f\"[INFO] The agent answered: {message}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren = Agent(persona=\"You are Warren Buffet, the famous investor. You are wise and give financial advice.\", tools=[get_stock_price_schema])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren.listen(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren.chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren.listen(\"What is the current stock price of Tesla?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the tool call response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, persona=None, tools=None):\n",
        "        self.chat_history = []\n",
        "        if persona:\n",
        "            self.chat_history.append({\"role\": \"system\", \"content\": persona})\n",
        "        if tools:\n",
        "            self.tools = tools\n",
        "\n",
        "    def listen(self, message):\n",
        "        self.chat_history.append({'role': 'user', 'content': message})\n",
        "        self._think()\n",
        "\n",
        "    def _think(self):\n",
        "        completion = client.chat.completions.create(\n",
        "            model='gpt-3.5-turbo',\n",
        "            messages=self.chat_history,\n",
        "            tools=self.tools \n",
        "        )\n",
        "        self._decide(completion)\n",
        "\n",
        "    def _use_tool(self, tool_call):\n",
        "        if tool_call.function.name == 'get_stock_price':\n",
        "            ticker = ast.literal_eval(tool_call.function.arguments)['ticker']\n",
        "            response = get_stock_price(ticker)\n",
        "            return str({\"output\": response})\n",
        "        return None\n",
        "\n",
        "    def _decide(self, completion):\n",
        "        response = completion.choices[0].message.content if completion.choices[0].message.content else None\n",
        "        tool_call = completion.choices[0].message.tool_calls if completion.choices[0].message.tool_calls else None\n",
        "        if response is not None and tool_call is None:\n",
        "            self._say(completion.choices[0].message.content)\n",
        "            self.chat_history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
        "            self._think()\n",
        "        if response is None and tool_call is not None:\n",
        "            print(\"[DEBUG] Tool call detected:\")\n",
        "            print(tool_call)\n",
        "            self.chat_history.append({\"role\": \"assistant\", \"tool_calls\": tool_call})\n",
        "            print(\"[DEBUG] Using tool...\")\n",
        "            tool_response = self._use_tool(tool_call[0])\n",
        "            self.chat_history.append({\"role\": \"tool\", \"content\": tool_response})\n",
        "            print(\"[DEBUG] Tool response:\", tool_response)\n",
        "            self._think()  \n",
        "        if response is not None and tool_call is not None:\n",
        "            print(\"[DEBUG] Both response and tool call detected, which is unexpected.\")\n",
        "            print(\"[DEBUG] Response:\", response)\n",
        "            print(\"[DEBUG] Tool call:\", tool_call)\n",
        "        if response is None and tool_call is None:\n",
        "            print(\"[DEBUG] No response or tool call detected, which is unexpected.\")\n",
        "            \n",
        "    def _say(self, message):\n",
        "        print(f\"[INFO] The agent answered: {message}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren = Agent(persona=\"You are Warren Buffet, the famous investor. You are wise and give financial advice.\", tools=[get_stock_price_schema])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren.listen(\"What is the current stock price of Tesla?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren.chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warren.listen(\"And what about Microsoft?\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
