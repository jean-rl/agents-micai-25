{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ1Y8bB5g8fC"
      },
      "source": [
        "Download the precompiled `llama-server` binary from the following github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z2l13h4fFei",
        "outputId": "77f5fc1c-8add-4ef5-d6fa-f6b06b190caa"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/jean-rl/llama-cpp-colab.git\n",
        "%cd llama-cpp-colab\n",
        "!chmod +x llama-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiLIA9w1C1gj"
      },
      "source": [
        "Then, download model weights in GGUF format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW-RADbMCykp",
        "outputId": "e1d04d46-3e40-4631-e82f-24acd2cba2e1"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWMCEVeuhvzO"
      },
      "source": [
        "Then open a terminal and run the following command.\n",
        "\n",
        "```\n",
        "./llama-server -m Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf --port 9090 -v --verbose-prompt --jinja -ngl 9999\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUq4jqLe6tDO"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_DINSXo6p2a"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI(\n",
        "    base_url=\"http://127.0.0.1:9090/v1\",\n",
        "    api_key = \"sk-no-key-required\"\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"Hi, how are you?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I am doing well, thank you! How can I assist you today?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you tell me a joke?\"},\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRpswiKx68Gh",
        "outputId": "30d5f1ab-9bb7-4d19-f3cb-386f85842317"
      },
      "outputs": [],
      "source": [
        "completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM9G5XzPDYfj",
        "outputId": "68a6f827-8626-4762-9793-bc4c0753f7ad"
      },
      "outputs": [],
      "source": [
        "print(completion.__verbose['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGbdm_UGDczX",
        "outputId": "6ef1b1d8-3873-409d-ea9e-d3261c3852d9"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DCiQ4VxESCa"
      },
      "source": [
        "#### Function calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIKn8qssDe86"
      },
      "outputs": [],
      "source": [
        "get_weather_schema =  {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Get current temperature for a given location.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\n",
        "                \"location\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je3B3xW7EX3V"
      },
      "outputs": [],
      "source": [
        "def get_weather(location):\n",
        "    print(f\"Getting weather for {location}...\")\n",
        "    # Simulate a weather API call\n",
        "    return f\"The current temperature in {location} is 25°C.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R2XsnvzEh8V"
      },
      "outputs": [],
      "source": [
        "chat_history = [\n",
        "        #{\"role\": \"system\", \"content\": \"You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal user question.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hi, what is the weather today in Bogota?\"}\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-Y4ZvzrEkHH"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=chat_history,\n",
        "    tools=[get_weather_schema]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT8NewvsEmcS",
        "outputId": "7f46fd20-9dfc-465e-e180-532f61a9f11f"
      },
      "outputs": [],
      "source": [
        "print(completion.__verbose['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHjGdMmkEnwh",
        "outputId": "c20336cc-2195-48a1-ef9d-13bf51207103"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGGdez1-ErlS",
        "outputId": "a70383b2-fb03-4378-fc62-3bb2813cc035"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.tool_calls[0].function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ18BKIrEvvs",
        "outputId": "89c8e649-f354-4bdd-a29f-6e6e681f8270"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "if completion.choices[0].message.tool_calls:\n",
        "    if completion.choices[0].message.tool_calls[0].function.name == 'get_weather':\n",
        "        response = get_weather(ast.literal_eval(completion.choices[0].message.tool_calls[0].function.arguments)['location'])\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "efC11-ckExzO",
        "outputId": "3ba60940-04b5-41b3-ba0a-0b252fb97047"
      },
      "outputs": [],
      "source": [
        "response = str({\"output\": response}) # If I don't return it in form of this dict. The model doesn't responds correctly. It's in the Llama documentation also https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufFGfoCME4sf"
      },
      "outputs": [],
      "source": [
        "chat_history.append({\"role\": \"assistant\", \"tool_calls\": completion.choices[0].message.tool_calls})\n",
        "chat_history.append({\"role\": \"tool\", \"content\": response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iLdR2hjE8OT",
        "outputId": "3b22cec5-5900-4f1f-d064-5e86d3c4a320"
      },
      "outputs": [],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij2Ma6RlFCLD"
      },
      "outputs": [],
      "source": [
        "chat_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal user question.\"}] + chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_c-famOFEM8",
        "outputId": "35469f9a-5bd0-4b5d-ae12-f5ebf6038252"
      },
      "outputs": [],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyPcYbiEFG-Y"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=chat_history,\n",
        "    #tools=[get_weather_schema] # If I include it. The model also responds but this contaminates the prompt and could harm performance\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXf1jtf3FINI",
        "outputId": "dd150254-21e5-4763-a9a4-077b999cb73c"
      },
      "outputs": [],
      "source": [
        "print(completion.__verbose['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtwPBY0gFOpc",
        "outputId": "4681817f-1e91-419e-88b4-c98e18349369"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "s1iMTIXrFSC2",
        "outputId": "76b83e1a-1b31-4166-8162-7ac290fe122c"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.tool_calls[0].function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1PnvJrLF23w"
      },
      "source": [
        "#### Building the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4aAOLFcF2tZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
