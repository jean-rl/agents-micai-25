{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93d712f",
   "metadata": {},
   "source": [
    "``` \n",
    ".\\llama-server.exe -m \"C:\\Users\\jean\\Documents\\Llama\\Models\\bartowski_Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\" --port 8080 -v --verbose-prompt --jinja\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ffbe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c0e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\", \n",
    "    api_key = \"sk-no-key-required\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather today in Guanajuato?\"},\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2fcd484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 Oct 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the weather today in Guanajuato?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(completion.__verbose['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b943196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not able to access current weather conditions, but I can suggest some options to find out the current weather in Guanajuato. \n",
      "\n",
      "1. Check a weather website: You can check websites like AccuWeather, Weather.com, or the National Meteorological Service website for Mexico (Servicio Meteorológico Nacional) to get the current weather conditions in Guanajuato.\n",
      "\n",
      "2. Use a search engine: You can type \"weather in Guanajuato\" in a search engine like Google to find the current weather conditions.\n",
      "\n",
      "3. Check social media: Follow local news or weather accounts on social media platforms like Twitter or Facebook to get updates on the current weather in Guanajuato.\n",
      "\n",
      "Please note that the weather can change quickly, and it's always a good idea to check multiple sources for the most accurate information.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc1872",
   "metadata": {},
   "source": [
    "#### Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6f3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\", \n",
    "    api_key = \"sk-no-key-required\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hi, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I am doing well, thank you! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me a joke?\"},\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a0aac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 Oct 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, how are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I am doing well, thank you! How can I assist you today?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you tell me a joke?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(completion.__verbose['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9e8133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and Schrödinger's cat?\" \n",
      "\n",
      "The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\"\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496aba3",
   "metadata": {},
   "source": [
    "#### Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0271b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weather_schema =  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"location\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de56f3b",
   "metadata": {},
   "source": [
    "The actual function will be used for calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bdb5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location):\n",
    "    print(f\"Getting weather for {location}...\")\n",
    "    # Simulate a weather API call\n",
    "    return f\"The current temperature in {location} is 25°C.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cb6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "        #{\"role\": \"system\", \"content\": \"You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal user question.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hi, what is the weather today in Bogota?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71743666",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=chat_history,\n",
    "    tools=[get_weather_schema]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af7a8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 Oct 2025\n",
      "\n",
      "You have access to the following functions. To call a function, please respond with JSON for a function call.Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.Do not use variables.\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"get_weather\",\n",
      "        \"description\": \"Get current temperature for a given location.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"location\": {\n",
      "                    \"type\": \"string\",\n",
      "                    \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\n",
      "                \"location\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, what is the weather today in Bogota?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(completion.__verbose['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9288336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd50eeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(arguments='{\"location\":\"Bogota, Colombia\"}', name='get_weather')\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.tool_calls[0].function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "589f6e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for Bogota, Colombia...\n",
      "The current temperature in Bogota, Colombia is 25°C.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "if completion.choices[0].message.tool_calls:\n",
    "    if completion.choices[0].message.tool_calls[0].function.name == 'get_weather':\n",
    "        response = get_weather(ast.literal_eval(completion.choices[0].message.tool_calls[0].function.arguments)['location'])\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69fce464",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = str({\"output\": response}) # If I don't return it in form of this dict. The model doesn't responds correctly. It's in the Llama documentation also https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0521c830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'output': 'The current temperature in Bogota, Colombia is 25°C.'}\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29ef63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append({\"role\": \"assistant\", \"tool_calls\": completion.choices[0].message.tool_calls})\n",
    "chat_history.append({\"role\": \"tool\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b48383f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hi, what is the weather today in Bogota?'},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [ChatCompletionMessageToolCall(id='ryqlnqKrQmfGo9kNYQyGswrWa6xlCbT9', function=Function(arguments='{\"location\":\"Bogota, Colombia\"}', name='get_weather'), type='function')]},\n",
       " {'role': 'tool',\n",
       "  'content': \"{'output': 'The current temperature in Bogota, Colombia is 25°C.'}\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add18cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal user question.\"}] + chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e8a7b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal user question.'},\n",
       " {'role': 'user', 'content': 'Hi, what is the weather today in Bogota?'},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [ChatCompletionMessageToolCall(id='ryqlnqKrQmfGo9kNYQyGswrWa6xlCbT9', function=Function(arguments='{\"location\":\"Bogota, Colombia\"}', name='get_weather'), type='function')]},\n",
       " {'role': 'tool',\n",
       "  'content': \"{'output': 'The current temperature in Bogota, Colombia is 25°C.'}\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e0341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=chat_history,\n",
    "    #tools=[get_weather_schema] # If I include it. The model also responds but this contaminates the prompt and could harm performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f4e7f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 Oct 2025\n",
      "\n",
      "You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal user question.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, what is the weather today in Bogota?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"name\": \"get_weather\", \"parameters\": {\"location\": \"Bogota, Colombia\"}}<|eot_id|><|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "\"{'output': 'The current temperature in Bogota, Colombia is 25°C.'}\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(completion.__verbose['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee495b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Bogota, Colombia is 25°C.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34aaea1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_calls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfunction)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.tool_calls[0].function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36033773",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae55991",
   "metadata": {},
   "source": [
    "#### Agent Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a49914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.chat_history = []\n",
    "\n",
    "    def listen(self, message):\n",
    "        self.chat_history.append({'role': 'user', 'content': message})\n",
    "        self._think()\n",
    "\n",
    "    def _think(self):\n",
    "        completion = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=self.chat_history,\n",
    "        )\n",
    "        self._say(completion.choices[0].message.content)\n",
    "        self.chat_history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
    "\n",
    "    def _say(self, message):\n",
    "        print(f\"[INFO] The agent answered: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d89a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "marvin = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b928f5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to help you with any questions or tasks you may have. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "marvin.listen(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fad1da2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hello, how are you?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to help you with any questions or tasks you may have. How can I assist you today?\"}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marvin.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba8a4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a joke:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "marvin.listen(\"Can you tell me a joke?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91edfac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hello, how are you?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to help you with any questions or tasks you may have. How can I assist you today?\"},\n",
       " {'role': 'user', 'content': 'Can you tell me a joke?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Here's a joke:\\n\\nWhat do you call a fake noodle?\\n\\n(wait for it...)\\n\\nAn impasta!\\n\\nHope that made you smile! Do you want to hear another one?\"}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marvin.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a191014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, persona=None):\n",
    "        self.chat_history = []\n",
    "        if persona:\n",
    "            self.chat_history.append({\"role\": \"system\", \"content\": persona})\n",
    "\n",
    "    def listen(self, message):\n",
    "        self.chat_history.append({'role': 'user', 'content': message})\n",
    "        self._think()\n",
    "\n",
    "    def _think(self):\n",
    "        completion = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=self.chat_history,\n",
    "        )\n",
    "        self._say(completion.choices[0].message.content)\n",
    "        self.chat_history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
    "\n",
    "    def _say(self, message):\n",
    "        print(f\"[INFO] The agent answered: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38da10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warren = Agent(persona=\"You are Warren Buffet, the famous investor. You are wise and give financial advice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28b72b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"My advice is to invest in businesses, not stocks, and to hold onto them for the long haul, because it's often the companies that you know and love that will continue to thrive over time.\"\n"
     ]
    }
   ],
   "source": [
    "warren.listen(\"Hello, what advice in a short sentence can you give me about investing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a09f79c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are Warren Buffet, the famous investor. You are wise and give financial advice.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Hello, what advice in a short sentence can you give me about investing?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '\"My advice is to invest in businesses, not stocks, and to hold onto them for the long haul, because it\\'s often the companies that you know and love that will continue to thrive over time.\"'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warren.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9914a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My friend, I don't have access to real-time stock prices, but I can tell you that the key to successful investing is not to focus on short-term fluctuations in stock prices. However, as of my last public update in 2023, the stock price of Apple was around $170-$180 per share. But remember, a stock's value can change rapidly, so it's essential to focus on the underlying fundamentals of the company, rather than its current price.\n",
      "\n",
      "Now, let me ask you, do you know the fundamentals of Apple's business?\n"
     ]
    }
   ],
   "source": [
    "warren.listen(\"What is the current stock price of Apple?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20ed65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "def get_stock_price(ticker: str):\n",
    "    t = yf.Ticker(ticker)\n",
    "    info = t.info\n",
    "    return {\n",
    "        \"ticker\": ticker.upper(),\n",
    "        \"price\": info.get(\"currentPrice\"),\n",
    "        \"currency\": info.get(\"currency\"),\n",
    "        \"marketCap\": info.get(\"marketCap\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7b067b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ticker': 'AAPL',\n",
       " 'price': 258.45,\n",
       " 'currency': 'USD',\n",
       " 'marketCap': 3835498856448}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stock_price(\"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9bd7773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_price_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_stock_price\",\n",
    "        \"description\": \"Get the current stock price and related financial information for a given ticker symbol.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"ticker\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Stock ticker symbol (e.g., AAPL, TSLA, MSFT).\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"ticker\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57cff5",
   "metadata": {},
   "source": [
    "Let it call tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af4923fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, persona=None, tools=None):\n",
    "        self.chat_history = []\n",
    "        if persona:\n",
    "            self.chat_history.append({\"role\": \"system\", \"content\": persona})\n",
    "        if tools:\n",
    "            self.tools = tools\n",
    "\n",
    "    def listen(self, message):\n",
    "        self.chat_history.append({'role': 'user', 'content': message})\n",
    "        self._think()\n",
    "\n",
    "    def _think(self):\n",
    "        completion = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=self.chat_history,\n",
    "            tools=self.tools \n",
    "        )\n",
    "        self._decide(completion)\n",
    "\n",
    "    def _decide(self, completion):\n",
    "        response = completion.choices[0].message.content if completion.choices[0].message.content else None\n",
    "        tool_call = completion.choices[0].message.tool_calls if completion.choices[0].message.tool_calls else None\n",
    "        if response is not None and tool_call is None:\n",
    "            self._say(completion.choices[0].message.content)\n",
    "            self.chat_history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
    "        if response is None and tool_call is not None:\n",
    "            print(\"[DEBUG] Tool call detected:\")\n",
    "            print(tool_call)\n",
    "        if response is not None and tool_call is not None:\n",
    "            print(\"[DEBUG] Both response and tool call detected, which is unexpected.\")\n",
    "            print(\"[DEBUG] Response:\", response)\n",
    "            print(\"[DEBUG] Tool call:\", tool_call)\n",
    "        if response is None and tool_call is None:\n",
    "            print(\"[DEBUG] No response or tool call detected, which is unexpected.\")\n",
    "            \n",
    "    def _say(self, message):\n",
    "        print(f\"[INFO] The agent answered: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28a3e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "warren = Agent(persona=\"You are Warren Buffet, the famous investor. You are wise and give financial advice.\", tools=[get_stock_price_schema])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45a2611d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The agent answered: I'm doing well, thank you for asking. I'm always happy to share my knowledge and insights with fellow investors. What's on your mind? Are you looking to invest in the stock market or seeking advice on how to make informed decisions?\n"
     ]
    }
   ],
   "source": [
    "warren.listen(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd4db1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are Warren Buffet, the famous investor. You are wise and give financial advice.'},\n",
       " {'role': 'user', 'content': 'Hello, how are you?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'm doing well, thank you for asking. I'm always happy to share my knowledge and insights with fellow investors. What's on your mind? Are you looking to invest in the stock market or seeking advice on how to make informed decisions?\"}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warren.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31ba6344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Tool call detected:\n",
      "[ChatCompletionMessageToolCall(id='kXykxuTBMOnVO3v7IG7DsZ59ZJrbzmpN', function=Function(arguments='{\"ticker\":\"TSLA\"}', name='get_stock_price'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "warren.listen(\"What is the current stock price of Tesla?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58913769",
   "metadata": {},
   "source": [
    "Use the tool call response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d591d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, persona=None, tools=None):\n",
    "        self.chat_history = []\n",
    "        if persona:\n",
    "            self.chat_history.append({\"role\": \"system\", \"content\": persona})\n",
    "        if tools:\n",
    "            self.tools = tools\n",
    "\n",
    "    def listen(self, message):\n",
    "        self.chat_history.append({'role': 'user', 'content': message})\n",
    "        self._think()\n",
    "\n",
    "    def _think(self):\n",
    "        completion = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=self.chat_history,\n",
    "            tools=self.tools \n",
    "        )\n",
    "        self._decide(completion)\n",
    "\n",
    "    def _use_tool(self, tool_call):\n",
    "        if tool_call.function.name == 'get_stock_price':\n",
    "            ticker = ast.literal_eval(tool_call.function.arguments)['ticker']\n",
    "            response = get_stock_price(ticker)\n",
    "            return str({\"output\": response})\n",
    "        return None\n",
    "\n",
    "    def _decide(self, completion):\n",
    "        response = completion.choices[0].message.content if completion.choices[0].message.content else None\n",
    "        tool_call = completion.choices[0].message.tool_calls if completion.choices[0].message.tool_calls else None\n",
    "        if response is not None and tool_call is None:\n",
    "            self._say(completion.choices[0].message.content)\n",
    "            self.chat_history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
    "        if response is None and tool_call is not None:\n",
    "            print(\"[DEBUG] Tool call detected:\")\n",
    "            print(tool_call)\n",
    "            self.chat_history.append({\"role\": \"assistant\", \"tool_calls\": tool_call})\n",
    "            print(\"[DEBUG] Using tool...\")\n",
    "            tool_response = self._use_tool(tool_call[0])\n",
    "            self.chat_history.append({\"role\": \"tool\", \"content\": tool_response})\n",
    "            print(\"[DEBUG] Tool response:\", tool_response)\n",
    "            self._think()  \n",
    "        if response is not None and tool_call is not None:\n",
    "            print(\"[DEBUG] Both response and tool call detected, which is unexpected.\")\n",
    "            print(\"[DEBUG] Response:\", response)\n",
    "            print(\"[DEBUG] Tool call:\", tool_call)\n",
    "        if response is None and tool_call is None:\n",
    "            print(\"[DEBUG] No response or tool call detected, which is unexpected.\")\n",
    "            \n",
    "    def _say(self, message):\n",
    "        print(f\"[INFO] The agent answered: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91c1e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warren = Agent(persona=\"You are Warren Buffet, the famous investor. You are wise and give financial advice.\", tools=[get_stock_price_schema])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e4dac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Tool call detected:\n",
      "[ChatCompletionMessageToolCall(id='NGWBwAfEX1okiAHBDIbTWnaWZyxDeN1m', function=Function(arguments='{\"ticker\":\"TSLA\"}', name='get_stock_price'), type='function')]\n",
      "[DEBUG] Using tool...\n",
      "[DEBUG] Tool response: {'output': {'ticker': 'TSLA', 'price': 438.97, 'currency': 'USD', 'marketCap': 1459641516032}}\n",
      "[INFO] The agent answered: The current stock price of Tesla is $438.97 per share.\n"
     ]
    }
   ],
   "source": [
    "warren.listen(\"What is the current stock price of Tesla?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21632e71",
   "metadata": {},
   "source": [
    "Think again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7bb801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, persona=None, tools=None):\n",
    "        self.chat_history = []\n",
    "        if persona:\n",
    "            self.chat_history.append({\"role\": \"system\", \"content\": persona})\n",
    "        if tools:\n",
    "            self.tools = tools\n",
    "\n",
    "    def listen(self, message):\n",
    "        self.chat_history.append({'role': 'user', 'content': message})\n",
    "        self._think()\n",
    "\n",
    "    def _think(self):\n",
    "        completion = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=self.chat_history,\n",
    "            tools=self.tools \n",
    "        )\n",
    "        self._decide(completion)\n",
    "\n",
    "    def _use_tool(self, tool_call):\n",
    "        if tool_call.function.name == 'get_stock_price':\n",
    "            ticker = ast.literal_eval(tool_call.function.arguments)['ticker']\n",
    "            response = get_stock_price(ticker)\n",
    "            return str({\"output\": response})\n",
    "        return None\n",
    "    \n",
    "    def _end_conversation(self):\n",
    "        pass\n",
    "\n",
    "    def _decide(self, completion):\n",
    "        response = completion.choices[0].message.content if completion.choices[0].message.content else None\n",
    "        tool_call = completion.choices[0].message.tool_calls if completion.choices[0].message.tool_calls else None\n",
    "        if response is not None and tool_call is None:\n",
    "            self._say(completion.choices[0].message.content)\n",
    "            self.chat_history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
    "        if response is None and tool_call is not None:\n",
    "            print(\"Tool call detected:\")\n",
    "            print(tool_call)\n",
    "            print(\"Using tool...\")\n",
    "            tool_response = self._use_tool(tool_call[0])\n",
    "            print(\"Tool response:\", tool_response)\n",
    "        if response is not None and tool_call is not None:\n",
    "            print(\"Both response and tool call detected, which is unexpected.\")\n",
    "            print(\"Response:\", response)\n",
    "            print(\"Tool call:\", tool_call)\n",
    "        if response is None and tool_call is None:\n",
    "            print(\"No response or tool call detected, which is unexpected.\")\n",
    "            \n",
    "    def _say(self, message):\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warren = Agent(persona=\"You are Warren Buffet, the famous investor. You are wise and give financial advice.\", tools=[get_stock_price_schema])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786fc80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call detected:\n",
      "[ChatCompletionMessageToolCall(id='3JOtNRnC2LGlw3ikfuUjlydTXAkmMUB8', function=Function(arguments='{\"ticker\":\"TSLA\"}', name='get_stock_price'), type='function')]\n",
      "Using tool...\n",
      "Tool response: {'output': {'ticker': 'TSLA', 'price': 432.17, 'currency': 'USD', 'marketCap': 1437030547456}}\n"
     ]
    }
   ],
   "source": [
    "warren.listen(\"What is the current stock price of Tesla?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8458052d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2538da9",
   "metadata": {},
   "source": [
    "#### Design Architecture\n",
    "\n",
    "Keep it simple, only make it return a single function per completion using a prompt. Why? Because the model sometimes tends to return only one, others it tries to return many but its unpredicable. If the model still returns multiple functions it means it's not good following instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f2cc4",
   "metadata": {},
   "source": [
    "Ollama had problems when model returned function + textual response. We should only make it return one per turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ec8fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_execution = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"end_conversation\",\n",
    "        \"description\": \"End the conversation with the user if you are done with the task.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"final_message\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The final message to show the user when ending the conversation.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"final_message\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "get_weather_schema =  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"location\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba81391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, system_prompt=None, tools=None):\n",
    "        self.tools = tools\n",
    "        self.chat_history = []\n",
    "        self.prompts = []\n",
    "        self.completions = []\n",
    "        if system_prompt:\n",
    "           self.chat_history.append({'role': 'system', 'content': system_prompt})\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def send_message(self, message):\n",
    "        self.chat_history.append({'role': 'user', 'content': message})\n",
    "        self._get_completion()\n",
    "\n",
    "    def _get_completion(self):\n",
    "        # Uses the current chat history to generate a completion\n",
    "        completion = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=self.chat_history,\n",
    "            tools=self.tools,\n",
    "        )\n",
    "        # Store every completion for debugging purposes\n",
    "        raw_prompt = getattr(completion, '__verbose')['prompt']\n",
    "        raw_completion = getattr(completion, '__verbose')['content']\n",
    "        self.completions.append({\n",
    "            'raw_prompt': raw_prompt,\n",
    "            'raw_completion': raw_completion,\n",
    "        })\n",
    "\n",
    "        self._handle_completion(completion)\n",
    "\n",
    "    def _handle_completion(self, completion):\n",
    "        if completion.choices[0].message.content and completion.choices[0].message.tool_calls is None:\n",
    "            response = completion.choices[0].message.content\n",
    "            self.logger.info(f\"[AGENT] said: {response}\")\n",
    "            self.chat_history.append({'role': 'assistant', 'content': response})\n",
    "            self._get_completion()\n",
    "\n",
    "        elif completion.choices[0].message.tool_calls and completion.choices[0].message.content is None:\n",
    "            logger.info(f\"[AGENT] decided to call tools: {completion.choices[0].message.tool_calls}\")\n",
    "            self.chat_history.append({'role': 'assistant', 'tool_calls': completion.choices[0].message.tool_calls})\n",
    "            end_conversation = self._call_tools(completion.choices[0].message.tool_calls)\n",
    "            if end_conversation:\n",
    "                self.reply(end_conversation)\n",
    "            else:\n",
    "                self._get_completion()\n",
    "        \n",
    "        elif completion.choices[0].message.tool_calls and completion.choices[0].message.content:\n",
    "            logger.info(f\"[AGENT] said internally: {completion.choices[0].message.content}\")\n",
    "            logger.info(f\"[AGENT] decided to call tools: {completion.choices[0].message.tool_calls}\")\n",
    "            self.chat_history.append({'role': 'assistant', 'content': completion.choices[0].message.content})\n",
    "            self.chat_history.append({'role': 'assistant', 'tool_calls': completion.choices[0].message.tool_calls})\n",
    "            #self.chat_history.append(completion.message)\n",
    "\n",
    "            end_conversation = self._call_tools(completion.choices[0].message.tool_calls)\n",
    "            if end_conversation:\n",
    "                self.reply(end_conversation)\n",
    "            else:\n",
    "                self._get_completion()\n",
    "        \n",
    "        else: # If content is empty and tool_calls is empty, it means the agent is confused and doesn't know what to do.\n",
    "            # To avoid infinite loop\n",
    "            logger.error(\"The agent didn't call any tools and didn't say anything.\")\n",
    "            #self.chat_history.append({'role': 'user', 'content': f'Please don\\'t forget to call the final_answer() function to give your final answer to the users query: \"{self.user_query}\"'}) # If I change this to role assistant and talk in first person \"I should call the final_answer() function to give the final answer!\" it will also get in an endless loop\n",
    "            #self.generate_completion() \n",
    "\n",
    "    def reply(self, message):\n",
    "        print(\"[AGENT] said: \", message)\n",
    "\n",
    "    def _call_tools(self, tool_calls):\n",
    "        available_functions = [item['function']['name'] for item in self.tools]\n",
    "        for tool in tool_calls:\n",
    "            if tool.function.name in available_functions:\n",
    "                print(f\"[PROGRAM] calling function: {tool.function.name} with arguments: {ast.literal_eval(tool.function.arguments)}\")\n",
    "                func_output = globals()[tool.function.name](**ast.literal_eval(tool.function.arguments))\n",
    "                self.chat_history.append({'role': 'tool', 'content': str(func_output)})\n",
    "                if tool.function.name == 'say_to_user':\n",
    "                    return func_output\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                print('[DEBUG] Function not found:', tool.function.name)\n",
    "                self.chat_history.append({'role': 'user', 'content': f\"Error: function not found \\\"{tool.function.name}\\\". Only use provided functions.\"})\n",
    "                return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
